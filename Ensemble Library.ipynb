{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "Introduction to Ensemble Learning\n",
    "\n",
    "* Basic Ensemble Techniques\n",
    "** 2.1 Max Voting\n",
    "** 2.2 Averaging\n",
    "** 2.3 Weighted Average\n",
    "* Advanced Ensemble Techniques\n",
    "** 3.1 Stacking\n",
    "** 3.2 Blending\n",
    "** 3.3 Bagging\n",
    "** 3.4 Boosting\n",
    "* Algorithms based on Bagging and Boosting\n",
    "** 4.1 Bagging meta-estimator\n",
    "** 4.2 Random Forest\n",
    "** 4.3 AdaBoost\n",
    "** 4.4 GBM\n",
    "** 4.5 XGB\n",
    "** 4.6 Light GBM\n",
    "** 4.7 CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Max Voting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict(x_test)\n",
    "pred2=model2.predict(x_test)\n",
    "pred3=model3.predict(x_test)\n",
    "\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(x_test)):\n",
    "    final_pred = np.append(final_pred, mode([pred1[i], pred2[i], pred3[i]]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = tree.DecisionTreeClassifier(random_state=1)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
    "model.fit(x_train,y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Averaging"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1+pred2+pred3)/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Weighted Average"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1*0.3+pred2*0.3+pred3*0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def Stacking(model,train,y,test,n_fold):\n",
    "   folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
    "   test_pred=np.empty((test.shape[0],1),float)\n",
    "   train_pred=np.empty((0,1),float)\n",
    "   for train_indices,val_indices in folds.split(train,y.values):\n",
    "      x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n",
    "      y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
    "\n",
    "      model.fit(X=x_train,y=y_train)\n",
    "      train_pred=np.append(train_pred,model.predict(x_val))\n",
    "      test_pred=np.append(test_pred,model.predict(test))\n",
    "    return test_pred.reshape(-1,1),train_pred"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = tree.DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=x_train,test=x_test,y=y_train)\n",
    "\n",
    "train_pred1=pd.DataFrame(train_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model2 = KNeighborsClassifier()\n",
    "\n",
    "test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=x_train,test=x_test,y=y_train)\n",
    "\n",
    "train_pred2=pd.DataFrame(train_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#本质：弱模型结果也进入train ,test\n",
    "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(df,y_train)\n",
    "model.score(df_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model1.fit(x_train, y_train)\n",
    "val_pred1=model1.predict(x_val)\n",
    "test_pred1=model1.predict(x_test)\n",
    "val_pred1=pd.DataFrame(val_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)\n",
    "\n",
    "model2 = KNeighborsClassifier()\n",
    "model2.fit(x_train,y_train)\n",
    "val_pred2=model2.predict(x_val)\n",
    "test_pred2=model2.predict(x_test)\n",
    "val_pred2=pd.DataFrame(val_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Combining the meta-features and the validation set, a logistic regression model is built to make predictions on the test set.\n",
    "df_val=pd.concat([x_val, val_pred1,val_pred2],axis=1)\n",
    "df_test=pd.concat([x_test, test_pred1,test_pred2],axis=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(df_val,y_val)\n",
    "model.score(df_test,y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 本质 只用validation和test 做meta features 建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, with replacement. The size of the subsets is the same as the size of the original set.\n",
    "\n",
    "Bagging algorithms:\n",
    "\n",
    "* Bagging meta-estimator\n",
    "* Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting algorithms:\n",
    "\n",
    "* AdaBoost\n",
    "* GBM\n",
    "* XGBM\n",
    "* Light GBM\n",
    "* CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#reading the dataset\n",
    "df=pd.read_csv('./Loan Prediction/train.csv')\n",
    "\n",
    "#filling missing values\n",
    "df['Gender'].fillna('Male', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=0)\n",
    "\n",
    "x_train=train.drop('Loan_Status',axis=1)\n",
    "y_train=train['Loan_Status']\n",
    "\n",
    "x_test=test.drop('Loan_Status',axis=1)\n",
    "y_test=test['Loan_Status']\n",
    "\n",
    "#create dummies\n",
    "x_train=pd.get_dummies(x_train)\n",
    "x_test=pd.get_dummies(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Bagging meta-estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging meta-estimator is an ensembling algorithm that can be used for both classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:\n",
    "\n",
    "1. Random subsets are created from the original dataset (Bootstrapping).\n",
    "2. The subset of the dataset includes all features.\n",
    "3. A user-specified base estimator is fitted on each of these smaller sets.\n",
    "4. Predictions from each model are combined to get the final result."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "model = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is another ensemble machine learning algorithm that follows the bagging technique. It is an extension of the bagging estimator algorithm. The base estimators in random forest are decision trees. Unlike bagging meta estimator, random forest randomly selects a set of features which are used to decide the best split at each node of the decision tree.\n",
    "\n",
    "Looking at it step-by-step, this is what a random forest model does:\n",
    "\n",
    "1. Random subsets are created from the original dataset (bootstrapping).\n",
    "2. At each node in the decision tree, only a random set of features are considered to decide the best split.\n",
    "3. A decision tree model is fitted on each of the subsets.\n",
    "4. The final prediction is calculated by averaging the predictions from all decision trees.\n",
    "\n",
    "To sum up, Random forest randomly selects data points and features, and builds multiple trees (Forest) ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# number of trees used\n",
    "print('Number of Trees used : ', model.n_estimators)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n",
    "\n",
    "Below are the steps for performing the AdaBoost algorithm:\n",
    "\n",
    "1. Initially, all observations in the dataset are given equal weights.\n",
    "2. A model is built on a subset of data.\n",
    "3. Using this model, predictions are made on the whole dataset.\n",
    "4. Errors are calculated by comparing the predictions and actual values.\n",
    "5. While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n",
    "6. Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n",
    "7. This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(random_state=1)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)\n",
    "0.81081081081081086"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model = AdaBoostRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Gradient Boosting (GBM)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)\n",
    "0.81621621621621621"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model= GradientBoostingRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘regularized boosting‘ technique.\n",
    "\n",
    "Let us see how XGBoost is comparatively better than other techniques:\n",
    "\n",
    "1. Regularization:\n",
    "* Standard GBM implementation has no regularisation like XGBoost.\n",
    "* Thus XGBoost also helps to reduce overfitting.\n",
    "2. Parallel Processing:\n",
    "* XGBoost implements parallel processing and is faster than GBM .\n",
    "* XGBoost also supports implementation on Hadoop.\n",
    "3. High Flexibility:\n",
    "* XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.\n",
    "* Handling Missing Values:\n",
    "* XGBoost has an in-built routine to handle missing values.\n",
    "4. Tree Pruning:\n",
    "* XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.\n",
    "* Built-in Cross-Validation:\n",
    "* XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)\n",
    "0.82702702702702702"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "model=xgb.XGBRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before discussing how Light GBM works, let’s first understand why we need this algorithm when we have so many others (like the ones we have seen above). Light GBM beats all the other algorithms when the dataset is extremely large. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset.\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import lightgbm as lgb\n",
    "train_data=lgb.Dataset(x_train,label=y_train)\n",
    "#define parameters\n",
    "params = {'learning_rate':0.001}\n",
    "model= lgb.train(params, train_data, 100) \n",
    "y_pred=model.predict(x_test)\n",
    "for i in range(0,185):\n",
    "   if y_pred[i]>=0.5: \n",
    "   y_pred[i]=1\n",
    "else: \n",
    "   y_pred[i]=0\n",
    "0.81621621621621621"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import lightgbm as lgb\n",
    "train_data=lgb.Dataset(x_train,label=y_train)\n",
    "params = {'learning_rate':0.001}\n",
    "model= lgb.train(params, train_data, 100)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse=mean_squared_error(y_pred,y_test)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.7 CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your categorical variables have too many labels (i.e. they are highly cardinal), performing one-hot-encoding on them exponentially increases the dimensionality and it becomes really difficult to work with the dataset.\n",
    "\n",
    "CatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from catboost import CatBoostClassifier\n",
    "model=CatBoostClassifier()\n",
    "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
    "model.fit(x_train,y_train,cat_features=([ 0,  1, 2, 3, 4, 10]),eval_set=(x_test, y_test))\n",
    "model.score(x_test,y_test)\n",
    "0.80540540540540539"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from catboost import CatBoostRegressor\n",
    "model=CatBoostRegressor()\n",
    "categorical_features_indices = np.where(df.dtypes != np.float)[0]\n",
    "model.fit(x_train,y_train,cat_features=([ 0,  1, 2, 3, 4, 10]),eval_set=(x_test, y_test))\n",
    "model.score(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
